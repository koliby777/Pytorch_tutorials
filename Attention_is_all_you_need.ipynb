{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMiraUkbrIuADWQyIPjs3r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koliby777/Pytorch_tutorials/blob/main/Attention_is_all_you_need.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention is all you need"
      ],
      "metadata": {
        "id": "Cq0xb5e_fxfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://monica.im/share/chat?shareId=DhWsggKYwHZdyjOC"
      ],
      "metadata": {
        "id": "KQGltFVLcmZG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGEAOy88anx6",
        "outputId": "ade5eaca-fcd6-4ff2-9bda-c4a99590a2bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 5, 8])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleAttention(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(SimpleAttention, self).__init__()\n",
        "        # Linear layer to transform the input into query vectors\n",
        "        self.query = nn.Linear(input_dim, hidden_dim)\n",
        "        # Linear layer to transform the input into key vectors\n",
        "        self.key = nn.Linear(input_dim, hidden_dim)\n",
        "        # Linear layer to transform the input into value vectors\n",
        "        self.value = nn.Linear(input_dim, hidden_dim)\n",
        "        # Softmax layer to normalize attention scores\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_length, input_dim)\n",
        "        # Transform the input tensor x into query vectors Q\n",
        "        Q = self.query(x)  # (batch_size, seq_length, hidden_dim)\n",
        "        # Transform the input tensor x into key vectors K\n",
        "        K = self.key(x)    # (batch_size, seq_length, hidden_dim)\n",
        "        # Transform the input tensor x into value vectors V\n",
        "        V = self.value(x)  # (batch_size, seq_length, hidden_dim)\n",
        "\n",
        "        # Calculate attention scores by performing dot product of Q and K\n",
        "        # Transpose K's last two dimensions for correct matrix multiplication\n",
        "        # Divide by the square root of the dimension of K to scale the scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(K.shape[-1], dtype=torch.float32))\n",
        "\n",
        "        # Apply softmax to the attention scores to get attention weights\n",
        "        # Softmax ensures the weights sum up to 1 and are positive\n",
        "        weights = self.softmax(scores)  # (batch_size, seq_length, seq_length)\n",
        "\n",
        "        # Compute the weighted sum of the value vectors V\n",
        "        # The weights determine how much focus each value vector gets\n",
        "        output = torch.matmul(weights, V)  # (batch_size, seq_length, hidden_dim)\n",
        "\n",
        "        # Return the output tensor, which now contains the attended information\n",
        "        return output\n",
        "\n",
        "# Example usage\n",
        "batch_size = 2  # Number of sequences in a batch\n",
        "seq_length = 5  # Length of each sequence\n",
        "input_dim = 10  # Dimensionality of the input features\n",
        "hidden_dim = 8  # Dimensionality of the hidden (transformed) features\n",
        "\n",
        "# Create a random input tensor with shape (batch_size, seq_length, input_dim)\n",
        "x = torch.randn(batch_size, seq_length, input_dim)\n",
        "# Instantiate the SimpleAttention module\n",
        "attention = SimpleAttention(input_dim, hidden_dim)\n",
        "# Pass the input tensor through the attention mechanism\n",
        "output = attention(x)\n",
        "# Print the shape of the output tensor\n",
        "print(output.shape)  # Should print: torch.Size([2, 5, 8])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed Explanation of Each Line\n",
        "Importing Libraries:\n",
        "\n",
        "python\n",
        "Copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "torch: The main PyTorch library for tensor operations.\n",
        "torch.nn: The sub-library for building neural networks.\n",
        "Defining the SimpleAttention Class:\n",
        "\n",
        "python\n",
        "Copy\n",
        "class SimpleAttention(nn.Module):\n",
        "class SimpleAttention: Defines a new class inheriting from nn.Module, the base class for all neural network modules in PyTorch.\n",
        "Initializing the Class:\n",
        "\n",
        "python\n",
        "Copy\n",
        "def __init__(self, input_dim, hidden_dim):\n",
        "    super(SimpleAttention, self).__init__()\n",
        "init: The constructor method to initialize the instance.\n",
        "super(SimpleAttention, self).init(): Calls the constructor of the parent class nn.Module.\n",
        "Defining Linear Layers:\n",
        "\n",
        "python\n",
        "Copy\n",
        "self.query = nn.Linear(input_dim, hidden_dim)\n",
        "self.key = nn.Linear(input_dim, hidden_dim)\n",
        "self.value = nn.Linear(input_dim, hidden_dim)\n",
        "nn.Linear(input_dim, hidden_dim): Creates a linear transformation layer.\n",
        "self.query, self.key, self.value: These layers transform the input into query, key, and value vectors respectively.\n",
        "Defining Softmax Layer:\n",
        "\n",
        "python\n",
        "Copy\n",
        "self.softmax = nn.Softmax(dim=-1)\n",
        "nn.Softmax(dim=-1): Creates a softmax layer to normalize the attention scores.\n",
        "Defining the Forward Method:\n",
        "\n",
        "python\n",
        "Copy\n",
        "def forward(self, x):\n",
        "forward(self, x): Defines the forward pass of the model, which computes the output given the input x.\n",
        "Transforming Input into Q, K, V:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Q = self.query(x)\n",
        "K = self.key(x)\n",
        "V = self.value(x)\n",
        "self.query(x), self.key(x), self.value(x): Apply the linear transformations to obtain the query, key, and value vectors.\n",
        "Calculating Attention Scores:\n",
        "\n",
        "python\n",
        "Copy\n",
        "scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(K.shape[-1], dtype=torch.float32))\n",
        "torch.matmul(Q, K.transpose(-2, -1)): Computes the dot product between Q and the transpose of K.\n",
        "/ torch.sqrt(torch.tensor(K.shape[-1], dtype=torch.float32)): Scales the scores by the square root of the dimension of K.\n",
        "Applying Softmax to Scores:\n",
        "\n",
        "python\n",
        "Copy\n",
        "weights = self.softmax(scores)\n",
        "self.softmax(scores): Normalizes the attention scores to get the attention weights.\n",
        "Computing the Weighted Sum of Vectors:\n",
        "\n",
        "python\n",
        "Copy\n",
        "output = torch.matmul(weights, V)\n",
        "torch.matmul(weights, V): Computes the weighted sum of the value vectors V using the attention weights.\n",
        "Returning the Output:\n",
        "\n",
        "python\n",
        "Copy\n",
        "return output\n",
        "return output: Returns the final attended output.\n",
        "Example Usage:\n",
        "\n",
        "python\n",
        "Copy\n",
        "batch_size = 2\n",
        "seq_length = 5\n",
        "input_dim = 10\n",
        "hidden_dim = 8\n",
        "x = torch.randn(batch_size, seq_length, input_dim)\n",
        "attention = SimpleAttention(input_dim, hidden_dim)\n",
        "output = attention(x)\n",
        "print(output.shape)  # Should print: torch.Size([2, 5, 8])\n",
        "batch_size, seq_length, input_dim, hidden_dim: Define the dimensions for the example.\n",
        "torch.randn(batch_size, seq_length, input_dim): Creates a random input tensor.\n",
        "SimpleAttention(input_dim, hidden_dim): Instantiates the attention module.\n",
        "attention(x): Passes the input tensor through the attention mechanism.\n",
        "print(output.shape): Prints the shape of the output tensor."
      ],
      "metadata": {
        "id": "lbegoyVva0Uu"
      }
    }
  ]
}